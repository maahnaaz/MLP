{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUOhRn2CG6ct"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import matplotlib as mlp\n",
        "import pandas as pd\n",
        "from numpy.random import RandomState\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krQXRA6lpY2n"
      },
      "source": [
        "# (It is O.K. to set these parameters directly within the source code, please do not implement a user interface for that.)\n",
        "n = 3 # Number of layers\n",
        "Learning_rate = [0.1, 0.05] # learning rate for each layer (excluding Input layer)\n",
        "iteration = 1000 # number of iterations\n",
        "layers_num_neuron = []\n",
        "Transfer_func = []\n",
        "Der_Transfer_func = []\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfW1Gr8qQefE"
      },
      "source": [
        "def FermiFunction(z):\n",
        "  # Logistic Function\n",
        "  f = 1/(1+np.exp(-z))\n",
        "  return f"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQOsbyCKVCX7"
      },
      "source": [
        "def FermiFunctionDerivative(z):\n",
        "  # Derivative of logistic function\n",
        "  f = FermiFunction(z)\n",
        "  fprime= f *(1 - f)\n",
        "  return fprime"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGW4z2i4MjqU"
      },
      "source": [
        "def tanh(z):\n",
        "  # Tanh\n",
        "  t = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "  return t"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnLkrGM5MjhB"
      },
      "source": [
        "def tanhDerivative(z):\n",
        "  # Derivative of Tanh \n",
        "  t = 1 - tanh(z)**2\n",
        "  return t"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cjnL1rKMjPb"
      },
      "source": [
        "def Identity(z):\n",
        "  # Identity function\n",
        "  return z"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suWoZnPVMi_h"
      },
      "source": [
        "def IdentityDerivative(z):\n",
        "  # Derivative of Identity function\n",
        "  return 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcsI1Sjd82U"
      },
      "source": [
        "def Feedforward(layers_num_neuron, X, weights):\n",
        "  # In this function, we compute the output of each layer by multiplying the neurons \n",
        "  # of the last layer with corresponding weights.\n",
        "  global Transfer_func\n",
        "  out_final = {}\n",
        "  net = np.zeros((len(layers_num_neuron)-1, np.max(layers_num_neuron)))\n",
        "  fnet = np.zeros((len(layers_num_neuron)-1, np.max(layers_num_neuron)))\n",
        "  for i in range(len(layers_num_neuron)-1):\n",
        "    if i == 0:\n",
        "      net[i][:layers_num_neuron[i+1]] = weights[i].dot(X)\n",
        "    else:\n",
        "      net[i][:layers_num_neuron[i+1]] = weights[i].dot(fnet[i-1][:layers_num_neuron[i]])\n",
        "    \n",
        "      \n",
        "    fnet[i][:layers_num_neuron[i+1]] = Transfer_func[i-1](net[i][:layers_num_neuron[i+1]])\n",
        "  \n",
        "  \n",
        "  return net, fnet\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGP2Z3iIilM"
      },
      "source": [
        "\n",
        "def Update(layers_num_neuron, weights, delta):\n",
        "  # In this function we are going to update weights by adding the deltas\n",
        "  for i in range(len(layers_num_neuron)-1):\n",
        "    weights[i] = weights[i] + delta[i]\n",
        "  return weights\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVqaoqRbfNrc"
      },
      "source": [
        "def Backpropagation(X, Y, layers_num_neuron, net, fnet, weights):\n",
        "  # In this function, the weights are trained using delta-rule\n",
        "  global Learning_rate, Transfer_func, Der_Transfer_func\n",
        "  delta = {}\n",
        "  error = 0\n",
        "  s_delta =  np.zeros((len(layers_num_neuron)-1, np.max(layers_num_neuron)))\n",
        "  # Computing delta for weights\n",
        "  for i in range(len(layers_num_neuron)-1, 0, -1):\n",
        "    # Computing error and delta for the hidden-last layer\n",
        "    if i+1 == len(layers_num_neuron):\n",
        "      error = Y - fnet[i-1][:layers_num_neuron[i]]\n",
        "      for j in range(len(error)): \n",
        "        # delta_weight = learning_rate * error * f'(net) * out_previous_layer\n",
        "        delta_last_neuron_j = Learning_rate[i-1]*error[j]*Der_Transfer_func[i-1](net[i-1][j])*fnet[i-2][:layers_num_neuron[i-1]]\n",
        "        s_delta[i-1][j] = error[j]*Der_Transfer_func[i-1](net[i-1][j])\n",
        "        if (i-1) in delta.keys():\n",
        "          delta[i-1].append(delta_last_neuron_j)\n",
        "        else:\n",
        "          delta[i-1] = [delta_last_neuron_j]\n",
        "    # Computing error and delta for weights between hidden layers and first-hidden layer\n",
        "    else:\n",
        "      weight = weights[i]\n",
        "      for j in range(layers_num_neuron[i]): \n",
        "        back_error = 0\n",
        "        # Backpropagating the error from next layer, Sh = (zigma Sk * Whk)*f'(net)\n",
        "        for k in range(layers_num_neuron[i+1]):\n",
        "          back_error = back_error + s_delta[i][k]*weight[k][j]\n",
        "        back_error = back_error*Der_Transfer_func[i-1](net[i-1][j])\n",
        "        # delta_weight = learning_rate * Sh * input (For weights between first-hidden layer)\n",
        "        if i-1 == 0:\n",
        "          delta_last_neuron_j = Learning_rate[i-1]*back_error*X\n",
        "        # delta_weight = learning_rate * Sh * out_previous_layer (For weights between hidden-hidden layer)\n",
        "        else:\n",
        "          delta_last_neuron_j = Learning_rate[i-1]*back_error*fnet[i-2][:layers_num_neuron[i-1]]\n",
        "        s_delta[i-1][j] = back_error\n",
        "        if (i-1) in delta.keys():\n",
        "          delta[i-1].append(delta_last_neuron_j)\n",
        "        else:\n",
        "          delta[i-1] = [delta_last_neuron_j]\n",
        "\n",
        "  return Update(layers_num_neuron, weights, delta)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTAXctavSNne"
      },
      "source": [
        "# Reading File 1\n",
        "df = pd.read_csv(\"PA-B_training_data_01.txt\", sep=\" \", header=None,skiprows=2)\n",
        "m = 1 #number of neurons in the last layer\n",
        "n = 2 #layers_number\n",
        "df1 = df.dropna(axis=1)\n",
        "df1.head()\n",
        "X=df1.iloc[:,:n].values\n",
        "Y=df1.iloc[:,n:(n+m)].values\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT6cjmGEUC28"
      },
      "source": [
        "# Reading File 2\n",
        "df = pd.read_csv(\"PA-B_training_data_02.txt\", sep=\" \", header=None,skiprows=2)\n",
        "m = 1 #number of neurons in the last layer\n",
        "n = 2 #layers_number\n",
        "df1 = df.dropna(axis=1)\n",
        "X=df1.iloc[:,:n].values\n",
        "Y=df1.iloc[:,n:(n+m)].values\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcGxd1xmUMtG"
      },
      "source": [
        "# Reading File 3\n",
        "df = pd.read_csv(\"PA-B_training_data_03.txt\", sep=' ' ,error_bad_lines=True, header=None, skiprows=2)\n",
        "m = 2 #number of neurons in the last layer\n",
        "n = 4 #layers_number\n",
        "df1 = df.dropna(axis=1)\n",
        "\n",
        "X = df1.iloc[:, :n].values\n",
        "Y = df1.iloc[:,n:(n+m)].values\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uugFbqhqU5h3"
      },
      "source": [
        "# Reading File 4\n",
        "\n",
        "values = open('PA-B_training_data_04.txt').read().split()\n",
        "m = 1 #number of neurons in the last layer\n",
        "n = 2 #layers_number\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(6, len(values), 3):\n",
        "  X.append([values[i], values[i+1]])\n",
        "  Y.append(values[i+2])\n",
        "\n",
        "X = np.array(X).astype(np.float)\n",
        "Y = np.array(Y).astype(np.float)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CurFXBtvV73p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1aee28ac-5263-4b02-f902-e175f35748c3"
      },
      "source": [
        "layers_num_neuron = [n+1, 3, m] # Number of layers (<= 4) and neurons in each layer \n",
        "Transfer_func = [FermiFunction, tanh] # learning rate for each layer (excluding Input layer)\n",
        "Der_Transfer_func = [FermiFunctionDerivative, tanhDerivative]# Derivative of Transfer function for each layer (excluding Input layer)\n",
        "\n",
        "W={}\n",
        "# Generate initial weights with the same seed \n",
        "for i in range(len(layers_num_neuron)-1):\n",
        "  prng = RandomState(i*1000)\n",
        "  W[i] = prng.uniform(low = -2.0 , high = 2.001 , size = (layers_num_neuron[i+1], layers_num_neuron[i]) ) \n",
        "\n",
        "if len(X) <= 1000:\n",
        "  e=[]\n",
        "  for k in range(iteration):\n",
        "    Global_Error, error = 0, 0\n",
        "    # Read one of the files first, then begin the training\n",
        "    for i in range(len(X)):\n",
        "      # Adding bias\n",
        "      x = np.array(list([1]) + list(X[i]))\n",
        "      net, fnet = Feedforward(layers_num_neuron, x, W)\n",
        "      error = error + (Y[i] - fnet[-1][:layers_num_neuron[-1]])**2\n",
        "      W = Backpropagation(x, Y[i], layers_num_neuron, net, fnet, W)\n",
        "    Global_Error = np.sum(error)/len(X)\n",
        "    e.append(Global_Error)\n",
        "    # Writing Global Error on learning curve.txt\n",
        "    if k == 0:\n",
        "      f = open(\"learning curve.txt\", \"w\")\n",
        "    else:\n",
        "      f = open(\"learning curve.txt\", \"a\")\n",
        "    string = 'Eppoch ' + str(k) + '==> Global_Error ' + str(Global_Error) + '\\n'\n",
        "    f.write(string)\n",
        "    f.close()\n",
        "\n",
        "  plt.plot(e, np.arange(iteration), 'r')\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Global_Error\")\n",
        "  plt.title(\"Learning Curve\")\n",
        "  plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}