# MLP
Training a MLP Neural Network using Backpropagation.


Team work project with https://github.com/behnamfani/Training-a-Neural-Network-using-Backpropagation/blob/main/README.md

In this project, we created a neural network, and based on the data files, we trained the network using Backpropagation from the scratch.

Error function = sum(y - h(x))^2 / # inputs which y is desired output and h(x) is the output of the network.

A user can change the number of hidden layers, the number of neurons in each hidden layer, and the transfer functions (Logistic Function, Tanh, Identity) in each layer. The output of the program is a learning curve and a .txt file that includes the global error of every epoch.

Learning curve for dataset 1:

![alt text](https://github.com/maahnaaz/MLP/blob/main/1.png)

Learning curve for dataset 2:

![alt text](https://github.com/maahnaaz/MLP/blob/main/2.png)

Learning curve for dataset 3:

![alt text](https://github.com/maahnaaz/MLP/blob/main/3.png)

Learning curve for dataset 4:

![alt text](https://github.com/maahnaaz/MLP/blob/main/4.png)


